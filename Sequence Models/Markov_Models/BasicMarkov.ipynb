{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f68838-bfaa-413b-865d-6038c2e5f1fd",
   "metadata": {},
   "source": [
    "<center><h1>𝓜𝓪𝓻𝓴𝓸𝓿 𝓒𝓱𝓪𝓲𝓷</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03dd37b-0104-4816-a9a8-26b2b2258f0a",
   "metadata": {},
   "source": [
    "[**Refer This(Wonderful)**](https://www.youtube.com/watch?v=1GKtfgwf3ig)\n",
    "<center><img src=\"https://cdn.buttercms.com/4NJTXOLRRQ2mziK4Qp4z\" width=\"200\".></center>\n",
    "\n",
    "A Markov chain is a random process with the Markov property. A random process or often called stochastic property is a mathematical object defined as a collection of random variables. A Markov chain has either discrete state space (set of possible values of the random variables) or discrete index set (often representing time) - given the fact, many variations for a Markov chain exists. Usually the term \"Markov chain\" is reserved for a process with a discrete set of times, that is a Discrete Time Markov chain (DTMC).\n",
    "\n",
    "`Markov chain don't have memory, if travelling from one state to another, it forget the previous state, so it choose random path to go the next future state`. \n",
    "\n",
    "**Discrete Markov Chain**\n",
    " discrete time Markov chain is a sequence of random variables X1, X2, X3, ... with the Markov property, such that the probability of moving to the next state depends only on the present state and not on the previous states. Putting this is mathematical probabilistic formula:\n",
    "\n",
    "Pr( Xn+1 = x | X1 = x1, X2 = x2, …, Xn = xn) = Pr( Xn+1 = x | Xn = xn)\n",
    "\n",
    "\n",
    "<center><img src=\"https://cdn.buttercms.com/I4B2wnMSAq3jHP9mdsp4\" width=\"800\".></center>\n",
    "\n",
    "**This Graph is called Directed Graph**\n",
    "\n",
    "## Properties: \n",
    "\n",
    "<center><h3>1. 𝐹𝓊𝓉𝓊𝓇𝑒 𝒮𝓉𝒶𝓉𝑒 𝒹𝑒𝓅𝑒𝓃𝒹𝓈 o𝓃𝓁𝓎 o𝓃 𝒞𝓊𝓇𝓇𝑒𝓃𝓉 𝒮𝓉𝒶𝓉𝑒 </h3></center>\n",
    "\n",
    "### $$ P(x n+1 = x | Xn = xn) $$\n",
    "\n",
    "<center><h3>2. 𝒯𝒽𝑒 𝓈𝓊𝓂 𝑜𝒻 𝓉𝒽𝑒 𝓌𝑒𝒾𝑔𝒽𝓉𝓈 𝑜𝒻 𝑜𝓊𝓉𝑔𝑜𝒾𝓃𝑔 𝒶𝓇𝓇𝑜𝓌𝓈 𝒻𝓇𝑜𝓂 𝒶𝓃𝓎 𝓈𝓉𝒶𝓉𝑒 𝒾𝓈 𝑒𝓆𝓊𝒶𝓁 𝓉𝑜 𝟣 (𝒷𝑒𝒸𝒶𝓊𝓈𝑒 𝒾𝓉 𝒾𝓈 𝓅𝓇𝑜𝒷𝒶𝒷𝒾𝓁𝒾𝓉𝓎) </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b3afa-dd83-4a18-92a2-917a3f8fadf5",
   "metadata": {},
   "source": [
    "### Transition Probabilities: \n",
    "\n",
    "Consider you need to take a probability of an each event for long events, when you find the probability for any long event you will end up with the **stationary** or **equilibrium state** (it mean your probability values not varies after certain number of caclculations, it remains same). So better way to find the probability of each event is using a **adjacency matrix (A)** it's also called **Transiton matrix (A)**. \n",
    "\n",
    "\n",
    "<center><img src=\"images/trans.png\" width=\"800\".></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f8bdc-3dbb-4b4f-aba7-6f404657ac3c",
   "metadata": {},
   "source": [
    "Markov chains is a statistical process, it's a sequence of events where the probabilities of te future only depend on the present. In simple terms, markov chains don't have memory where it came from, it chooses the future path randomly! \n",
    "\n",
    "\n",
    "<center><img src=\"images/step1.png\" width=\"400\".></center>\n",
    "\n",
    "<center><img src=\"images/step2.png\" width=\"400\".></center>\n",
    "\n",
    "<center><img src=\"images/step3.png\" width=\"400\".></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
