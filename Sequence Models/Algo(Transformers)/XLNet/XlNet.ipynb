{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d3e4ba-1711-430c-84c3-669fa39c15b8",
   "metadata": {},
   "source": [
    "<center><h1>ğ“–ğ“®ğ“·ğ“®ğ“»ğ“ªğ“µğ“²ğ”ƒğ“®ğ“­ ğ“ğ“¾ğ“½ğ“¸ğ“»ğ“®ğ“°ğ“»ğ“®ğ“¼ğ“¼ğ“²ğ“¿ğ“® ğ“Ÿğ“»ğ“®ğ“½ğ“»ğ“ªğ“²ğ“·ğ“²ğ“·ğ“° ğ“¯ğ“¸ğ“» ğ“›ğ“ªğ“·ğ“°ğ“¾ğ“ªğ“°ğ“® ğ“¤ğ“·ğ“­ğ“®ğ“»ğ“¼ğ“½ğ“ªğ“·ğ“­ğ“²ğ“·ğ“°</h1></center>\n",
    "\n",
    "[**Reachers Paper**](https://arxiv.org/pdf/1906.08237.pdf) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf3d12-8f57-4a02-8a35-c7a45a7b6b2d",
   "metadata": {},
   "source": [
    "This is the first model,that beat the previous **SOTA** model **BERT**, you can consider this model like **Elephant inside the room**. \n",
    "\n",
    "It outperforms the **BERT** 18 out of 20 tasks, the intresting part is that, the architecture is similar to the BERT model but differnt is pretraining procedure.There are two types of unsupervised pretraining approach in the language modeling: \n",
    "\n",
    "<center><img src=\"https://i0.wp.com/researchdatapod.com/wp-content/uploads/2020/07/LM.gif?resize=480%2C196&ssl=1\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "1. AutoRegressive(AR)  \n",
    "2. AutoEncoding(AE)   \n",
    "\n",
    "**AutoRegressive(AR)**: \n",
    "* Helps to find the next word (auto regressive is because each token look at the previous tokens)\n",
    "\n",
    "**AutoEncoding(AE)**: \n",
    "* Consider this is the sentence: \"My name is aravind, I love the NLP\" \n",
    "* Then I change the sentnece to: \"My <\"predict\"> is aravind, I <\"predict\"> the NLP\"\n",
    "* Here, I want to predict the <\"predict\"> tag, this is autoencoding. \n",
    "* This was used by the **BERT** model, it gives the successfull understanding of the language. \n",
    "\n",
    "<center><img src=\"https://imgs.developpaper.com/imgs/4055417039-5e704fdcc441a_articlex.gif\" width=\"600\"/></center>\n",
    "\n",
    "**Here the most intresting part comes, XLnet want to use advantage of both the Unsupervised pretraining approach.**\n",
    "\n",
    "Let's see the difference between both of the methods: \n",
    "\n",
    "<center><img src=\"images/diff.png\" width=\"600\"></center>\n",
    "\n",
    "Here, you can see, the Bert produces the output **New** and **York** is completley independent with each other. They don't have somehow relationship with each other probabilities, it causes the wrong word formation problem. But if you see in XLnet, it takes cares of previously predicted output and inherit the output to the input, this is regressive modeling. \n",
    "\n",
    "**HOw it's done?**\n",
    "\n",
    "I already told you right? It uses the advantage of both the Bert and AutoRegressive! \n",
    "\n",
    "**small recap on AE and AR:** Sentence \"My name is <\"predict\"> and I eat..\", if I want to predict this by auto-regressive I can see \"My name is\", but If I want to predict this by using BERT I can see \"My name is ---- I eat\". In summary, bert see both the sides whereas the auto-regressive sees only the left side of the sentences. \n",
    "\n",
    "This is the formula for autoregressive, you see the formula, it able to predict the word by n-1 (x<t) sentence, In particularly, It can see only one direction. \n",
    "\n",
    "<center><img src=\"images/auto .png\" width=\"600\"></center> \n",
    "\n",
    "There are another type of autoregressive model is available, same as this but it can see the only right direction, but previous see only left direction. \n",
    "\n",
    "<center><img src=\"images/auto1.png\" width=\"600\"></center>\n",
    "\n",
    "Now, we have seen the small recap on AE and AR. How XLnet uses both the advantages. Here, AE sees both the direction whereas the AR sees only one direction. The main idea of the auto regressive model is that uses both the advantages, bro? How? Let's see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eff167-9e9f-4098-a84b-ec2c3fa7d4f2",
   "metadata": {},
   "source": [
    "**How it uses Both the advantages?**\n",
    "* AutoRegressive masks the 20% of the data, whereas the AE masks 15% of the data. \n",
    "* In XLnet we uses the AR method, I mean we mask the 20% of the words. \n",
    "* What we do means? Let's go to step by step! \n",
    "\n",
    "**Step1** \n",
    "\n",
    "* Sentences: \"My(1) name(2) is(3) aravind(4) and(5) I(6) Love(7) <to(8)> <eat(9)>\"\n",
    "* Here we masks the tag sentence and try to find by AR way. just see the left side. \n",
    "* step1.1 It predicts the word **to**: \"My(1) name(2) is(3) aravind(4) and(5) I(6) Love(7) to(8) <eat(9)>\", Now we need to predict the next word and the previous output is given to the input. \n",
    "* Stpe1.2:  \"My(1) name(2) is(3) aravind(4) and(5) I(6) Love(7) to(8) eat(9)\", Now it's predicts all the words, by sees only one direction (left). \n",
    "\n",
    "**Step2**: \n",
    "\n",
    "* Now let's change the order of the sentences: \n",
    "* Sentences: \"My(3) name(1) is(5) aravind(6) and(2) I(4) <\"Love(9)\"> to(7) eat(8).\"\n",
    "* Now let's predict the masked sentences by using the ordering. \n",
    "\n",
    "\n",
    "**Step3**: \n",
    "\n",
    "* Now let's change the order of the sentences same like previous and predict the word in different orders.  \n",
    "* Sentences: \"My(3) name(1) is(5) aravind(6) and(2) I(4) <\"Love(9)\"> to(7) eat(8).\"\n",
    "* Now let's predict the masked sentences by using the ordering. You can see here the masked word in **love**, now it having access of both the sides. How? \n",
    "* Because we trained the same sentence in previous steps, so it learns this sentence before step itself, So XLnet predicts the inner mask output by seeing both the direction! \n",
    "* This was done by trainig sentence with different ordering for many times. Now it can able to predict the sentence by referrig both the sides. \n",
    "\n",
    "\n",
    "Now it's having the advantage of both the **AR** & **AE**. This is how we are doing the pretraining using both the advantage of AE AND AR. Basically you are getting the advantage of BERT and doing the pretraining in AutoRegressive way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12d34a-5ea8-408d-a21f-8541d5ff1ac1",
   "metadata": {},
   "source": [
    "**Let's see Mathematically!**\n",
    "\n",
    "**AR Formula**\n",
    "\n",
    "<center><img src=\"images/AR.png\" width=\"600\"></center>\n",
    "\n",
    "It uses the log probability of the previous word `X<t` and it sees only the left side direction. \n",
    "\n",
    "**AE Formula**\n",
    "\n",
    "<center><img src=\"images/AE.png\" width=\"600\"></center>\n",
    "\n",
    "It just giving the approximate value of the predicted word and it sees all the direction `X cap` (It means all the context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3680fc-0b0d-4f26-a11b-824670e3b40d",
   "metadata": {},
   "source": [
    "**XLNET Formula**\n",
    "\n",
    "Here it decomposes the log probability to the sum of log probability given the words and predict the word by chosen permutations z and z is sampled uniformly from the set of all possible permutations. \n",
    "\n",
    "<center><img src=\"images/final.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3688b-7ead-4d97-958d-1e8e006a88bf",
   "metadata": {},
   "source": [
    "**Let's see Visually** \n",
    "\n",
    "See the factorization order for every diagram. \n",
    "\n",
    "<center><img src=\"images/visual.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc8217-ec12-4622-b042-3a390fea73c2",
   "metadata": {},
   "source": [
    "In order to achieve like this, we need to change the architecture because if you see the predicton word is x3, you not only predict this word only. In XLNet it masks 20% of the data, so you need to predict more words but the drawback is that, if you don't know previous word how do you predict the current word. I  mean XLNet is a autoregressive so it look only one direction, if you have a word to predict inbetween and word to predict in last, without knowing the first predicting word, you can't know the last prediciting word. If it goes like this it needs lot of training time. In order to achieve to look both the side and autoregressive, we need a architecutre change by adding **Masked Two Stream Attention**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d9652-b039-49f2-b225-24e18696e772",
   "metadata": {},
   "source": [
    "**Mathematically!**\n",
    "\n",
    "<center><img src=\"images/new.png\" width=\"600\"></center>\n",
    "\n",
    "Here we are going to create **two hidden states**, one is **G** and then**H**. **G** is allowded to look **previous states except his own state**, whereas the **H** is allowded to look **previous states including his own state**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a5227-390d-4fcf-a1bf-97fd8e33db77",
   "metadata": {},
   "source": [
    "**Visually!** \n",
    "\n",
    "<center><img src=\"images/bo.png\" width=\"800\"></center>\n",
    "\n",
    "Here we are going to predict the word by just the **G** there is no other hidden states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928691a-68ff-4c3d-a12e-bc41b3b0ca4a",
   "metadata": {},
   "source": [
    "Remaining all of this things are based on normal **Transformer XL** and some few improvements. One of the improvement is **memory thing** (Nothing but if you predict anything, you can save it somewhere. If you are predicting something in future, you can use the memory to reference for predicting). That is called the **mem block**. The hidden representation (predicted word) stored in the **Mem block**. and they used **reletive segment encodings**. \n",
    "\n",
    "**Some Comparison to other networks**\n",
    "\n",
    "<center><img src=\"images/seenu.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a2f12-c1dd-40fc-b792-da7a6e166c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
