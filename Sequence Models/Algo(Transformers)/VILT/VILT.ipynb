{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf3b20f-b9e8-49bd-9be5-419c6c61e280",
   "metadata": {},
   "source": [
    "<center><h1>ｖｉｓｉｏｎ ａｎｄ ｌａｎｇｕａｇｅ ｔｒａｎｓｆｏｒｍｅｒ</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa393d-48b2-4330-910f-e6e053e0d948",
   "metadata": {},
   "source": [
    "There are two most common form of data in the web. \n",
    "1.`Visual` \n",
    "2. `Text` \n",
    "Each can be processed by different methods. Some of them are. \n",
    "\n",
    "<center><img src=\"images/uni.png\" width=\"800\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e503de0-112c-4a32-96d5-3f74140eb6f4",
   "metadata": {},
   "source": [
    "`Patch Embeddings` we already seen in `vision transformers`. \n",
    "\n",
    "This are all `unimodal` models, if you want to combine both the unimodal you can use `modality interaction` that helps to combine both models. \n",
    "\n",
    "<center><img src=\"images/mul.png\" width=\"800\"/></center> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b49ed3-b187-4697-8478-ba15613fe155",
   "metadata": {},
   "source": [
    "There are four types of vision and language models: \n",
    "\n",
    "<center><img src=\"images/diff.png\" width=\"800\"/></center> \n",
    "\n",
    "\n",
    "The `size` of the box `indicates computational power` of the vision and language models. Our model is final model that consist of `very less computational` model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3366e0f-e61f-48b8-ac5a-cad591e4bd5e",
   "metadata": {},
   "source": [
    "Let's see the difference between 3rd and 4th model in the preceding image. (4th model is our model) \n",
    "\n",
    "<center><img src=\"images/dif.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f8272-d2bd-4d0b-9993-4d965de4336b",
   "metadata": {},
   "source": [
    "## Architecture Indepth: \n",
    "\n",
    "<center><img src=\"images/archi.png\" width=\"1000\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d7484-6ed0-4203-8d4e-267b50b14d8b",
   "metadata": {},
   "source": [
    "- **`Tranformer Encoder`** is a **`VIT BASE`** patch size of `32x32` \n",
    "\n",
    "### PreTraining: \n",
    "* The model is pre-trained with four different datsets and three different strategies. \n",
    "* And trained `200k` steps with `4k` batch size. \n",
    "\n",
    "<center><img src=\"images/pre.png\" width=\"600\"/></center> \n",
    "\n",
    "We do thre `Pre-Training` strategies. `MLM`-> Masked language modeling, `ITM` -> Image Text Matching, \n",
    "\n",
    "\n",
    "\n",
    "**`Masked Language Modeling`** \n",
    "- Just a MLM what we have studied in normal transformers models. \n",
    "\n",
    "\n",
    "\n",
    "**`Image Text Matching`** \n",
    "\n",
    "- It is a binary classification. \n",
    "- It outputs `True` if the input `image-text-pair` is aligned or false. \n",
    "\n",
    "\n",
    "**`Word Patch Alignment`**\n",
    "\n",
    "- It basically measures the distance between sets in our case `textual set` and `visual set`. \n",
    "- The distance should be minimized when the `image text pair` is aligned and should be maximized when it is not. \n",
    "\n",
    "<center><img src=\"images/pres.png\" width=\"900\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809615c-b621-4df6-8b9d-7ce2d556bffc",
   "metadata": {},
   "source": [
    "### Fine Tunning: \n",
    "* We evaluate the model using four datasets with the mini batch size of `256` \n",
    "* This datasets are based on `retrival` and `classification` tasks. \n",
    "\n",
    "<center><img src=\"images/fine.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab601ecd-cbd0-422b-bb01-5c4e393e043b",
   "metadata": {},
   "source": [
    "## Reference: \n",
    "* [**VILT Paper Talk**](https://papertalk.org/papertalks/32619)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
