{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089fc14c-d7d1-4d36-9a39-5f4ac3f5f215",
   "metadata": {},
   "source": [
    "<center><h1>Bidirectional Auto Regressive Transformers 29 OCT 2019 </h1></center>\n",
    "\n",
    "[**Research Paper**](https://arxiv.org/pdf/1910.13461.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94280403-9a2d-4685-80e3-567fb1b13a11",
   "metadata": {},
   "source": [
    "**Why BART?**\n",
    "* Normal BERT can't do all the task like question answsering, translation, summarization but here we are going to see about the BART, it can do classification, token classificaiton, question answering, translation, summarization (Basically every kind of NLP tasks). \n",
    "* It consist of Encoder and Decoder architecture (basically a normal architecutre of the original transformer). \n",
    "* Similar to Robert approach and this is the **SOTA** model for many text generation tasks. \n",
    "\n",
    "\n",
    "**Denoising AutoEncoders**\n",
    "\n",
    "* Here BART is kind of a Denoising AutoEncoders. \n",
    "* It's just like a BERT it randomly mask the input data to 50%. \n",
    "* It's just a maksed langauge modeling. \n",
    "* You can say MSP is a Denoising AutoEncoders: \n",
    "\n",
    "\n",
    "**Let's see some difference between (BERT)Encoder & (GPT)DECODER**\n",
    "<center><img src=\"images/diff.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179fc56-58fb-41e7-8855-893148b2eb17",
   "metadata": {},
   "source": [
    "**Here we are going to combine both the features of ENCODER & DECODER (original Transformer)** \n",
    "\n",
    "# BART MODEL: \n",
    "<center><img src=\"images/BART.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950915ae-2d1e-43f9-a69d-9fdc52b4bd81",
   "metadata": {},
   "source": [
    "* BART Encoder is a Bidrectional Encoder and Decoder is a Autoregressive Decoder (It combines the BERT AND GPT to perform well). \n",
    "\n",
    "### PreTraining: \n",
    "* This is a noraml architecture of transformer. What special? \n",
    "* Yes, something special here, because the way they pretrianing the model is so smart. \n",
    "* Let's see some of the self supervised pre training in detail. \n",
    "\n",
    "<center><img src=\"images/pre.png\" width=\"800\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f809753-5059-48dd-8365-df2ccbcf6711",
   "metadata": {},
   "source": [
    "#### Token Masking: \n",
    "* Same as the MSP(BERT), Nothing difference \n",
    "\n",
    "#### Token Deletion: \n",
    "* Random tokens are deleted from the sentence and the model need to decide which position are missing and impute the values. \n",
    "* It is the hard task to learn, but if it will that's the wonderful moment. \n",
    "\n",
    "#### Text Infilling: \n",
    "* Text infilling teaches the model to predict how many tokens are missing from a span.\n",
    "* It is very similar to token masking(BERT)(it randomly masking) if bert masked 3 tokens, you have 3 predicted tokens. But if BART masked 3 tokens it has one token containing all the 3 of it. \n",
    "\n",
    "### Sentence Permutation: \n",
    "* A document is divided into sentences based on full stops, and these sentences are shuffled in a random order. \n",
    "* The model need to predict the random order to original order \n",
    "\n",
    "#### Document Rotation: \n",
    "* A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document.\n",
    "* In simple words, `I love you`, random token = `you`, Sentence now = `you I love1`, Here model need to predict the order of the sentence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8315860-192e-4e40-969d-2dc1c51c4caa",
   "metadata": {},
   "source": [
    "### Results: \n",
    "<center><img src=\"images/results.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37043754-6f53-4715-9a6f-79979daffcbb",
   "metadata": {},
   "source": [
    "Header is different datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
