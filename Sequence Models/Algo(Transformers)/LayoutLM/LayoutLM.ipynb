{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6e2fb9-edf8-483b-85a0-218ee4049fb5",
   "metadata": {},
   "source": [
    "<center><h1>ùìõùì™ùîÇùì∏ùìæùìΩùìõùìú ùìø2</h1></center>\n",
    "\n",
    "[**Reserach Paper**](https://arxiv.org/pdf/2012.14740v4.pdf) \n",
    "\n",
    "[**Codes**](https://github.com/karndeepsingh/Extract_key_information_Document_understanding/blob/main/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP_(using_LayoutLMv2Processor).ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dbffe-520d-448e-91b0-7e26de77cd8d",
   "metadata": {},
   "source": [
    "* **LayoutL**anguage**M**odel is used to classify the documents to its respective category. It needs training data (independent and dependent) then we train a model as usual! \n",
    "* In case of Document understanding the LayoutLM doing a great job in recent years. \n",
    "* `Microsoft` has released this model. \n",
    "\n",
    "\n",
    "**Why we are using this, why not normal image classifier?**\n",
    "\n",
    "* If you look at normal image classifier like resnet or vgnet we give the training data and get the classified output of that, in case of image it's fine. But we are using the text inside it. (Here we are not able to utilize the text inside the image) \n",
    "* So, we don't need to classify the image based on patterns, we need to understand the text present inside the image. Then only we can classify the document in very effective manner. \n",
    "* Researchers found a new way to understand the text as well as understand the document layout using in the image using LayoutLM model. \n",
    "* This model helps to understand the doucument layout and document text then it classify that which category this image belongs to. \n",
    "* Intresting part is that, we get image embedding and text embedding and give to the model for training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d029d8-5512-42d7-91e9-94550f5832bd",
   "metadata": {},
   "source": [
    "### Architecture: \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/875/1*yfsiqeTguo-qx-cXxpppJw.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba58fdd-7eb1-4a90-9c67-959d46710f3e",
   "metadata": {},
   "source": [
    "* Image embeddings are taken `Feature maps` and text are also taken `OCR` from the image.\n",
    "* `2D image embeddings` the position of the text in the `image` (Bounding box of the image) \n",
    "* It learns image embedding and text embedding. \n",
    "\n",
    "1. `Word Embedding1`: `Wordpiece` algoritm  to tokenize the text from `OCR`\n",
    "2. `Visual Embedding`: `CNN` we take the output of the `CNN Encoder`, which converts the page image to a fixed-length sequence. We use ResNeXt-FPN ( architecture as the backbone of the visual encoder, whose parameters can be updated through backpropagation. \n",
    "3. `1D Positional Embedding`: Since\n",
    "the CNN-based visual backbone cannot capture the\n",
    "positional information, we also add a 1D positional\n",
    "embedding to these visual token embeddings. The\n",
    "1D positional embedding is shared with the text\n",
    "embedding layer. For the segment embedding, we\n",
    "attach all visual tokens to the visual segment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c03cd-6c21-4ee8-84ea-b539e271e9ec",
   "metadata": {},
   "source": [
    "### PreTraining tasks: \n",
    "\n",
    "#### 1. **`Masked Visual Language Modeling`**\n",
    "* It randomly mask the token and ask the model to find the token. \n",
    "* The output of this fed to classifier with vocabulary and cross entorpy loss to avoid visual clue leakage. \n",
    "\n",
    "#### 2. **`Text-Image Alignment`**\n",
    "* To help the model learn\n",
    "the spatial location correspondence between image\n",
    "and coordinates of bounding boxes, we propose\n",
    "the Text-Image Alignment (TIA) as a fine-grained\n",
    "cross-modality alignment. \n",
    "* \n",
    "\n",
    "\n",
    "#### 3. **`Text Image Matching`** \n",
    "* Text-Image Matching Furthermore, a coarsegrained cross-modality alignment task, Text-Image\n",
    "Matching (TIM) is applied to help the model learn\n",
    "the correspondence between document image and\n",
    "textual content. We feed the output representation\n",
    "at [CLS] into a classifier to predict whether the\n",
    "image and text are from the same document page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89830142-d8d3-46d1-a818-198ba801d0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
