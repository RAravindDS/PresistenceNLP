{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535f79ac-dea2-45b3-8e2a-4254c4ec4e93",
   "metadata": {},
   "source": [
    "<center><h1>ùì•ùì≤ùìºùì≤ùì∏ùì∑ ùì£ùìªùì™ùì∑ùìºùìØùì∏ùìªùì∂ùìÆùìª</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e155bb-e4dc-428f-beb3-4e85316427d0",
   "metadata": {},
   "source": [
    "For image classification purpose we use normal **`CNN`** architecture that helps to train and inferene easily. One of the best model for classification is **`resnet`**. We stack more resnet layers and able to get the better output by using **`resnet`**. But what if transformer architecture helps to classify the images better than resnet?  Yes, we train the transformer model for `computer vision` tasks and able to get more accuracy compare to the `resnet` model. \n",
    "\n",
    "Let's look how normal trainig and inference happen by an image: \n",
    "<center><img src=\"images/normal .png\" width=\"600\"/></center>\n",
    "\n",
    "Transfomer model is build for `NLP` tasks then people build it for `computer vision` tasks also. It is better than normal `cnn` models, it outperforms many datasets (when it has more than **`100`** million data). Let's understand this by diagram. \n",
    "\n",
    "<center><img src=\"images/im.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe24a2-9b62-4dc2-b2a0-ddc8877160cc",
   "metadata": {},
   "source": [
    "So, basically we need a `very large amount of data` to train the transformer model, otherwise resnet beats the transformer. So people have trained the transformer model with various datasets. Datasets are \n",
    "\n",
    "<center><img src=\"images/datasets.png\" width=\"600\"/></center>\n",
    "\n",
    "The JFT dataset is only available to `Google`. It's not available for public."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13abe88-2065-47aa-8492-e070faeaedd0",
   "metadata": {},
   "source": [
    "After training this we can use this model for our task `fine tunning`. We need to train the model for own dataset to learn classify better. Let's understand this by an image below: \n",
    "<center><img src=\"images/fine.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd2d13-67dd-4968-bb79-310a26b761a7",
   "metadata": {},
   "source": [
    "Let's understand how `VIT` works~ \n",
    "\n",
    "## Architecture of Vision Transformer\n",
    "\n",
    "\n",
    "### 1. Input Data Processing: \n",
    "In normal NLP tasks, we convert our `text` to `word embeddigns`. Here we need to find a way like preceding thing. So reserchers found that breaking the images into `non overlapping patches` then send to model. How this is exactly happening? Let's understand this by an image: \n",
    "\n",
    "<img src=\"images/wp.png\" width=\"300\"/>   <img src=\"images/wip.png\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c54ba7-72cf-40a2-9bca-b37df703f23c",
   "metadata": {},
   "source": [
    "You can overlap the images, but in reseracher paper they didn't overlapped. Here you need to specify two things\n",
    "1. Stride size \n",
    "2. Patch size \n",
    "\n",
    "`Stride size` -> Just a striding window It tells what steps patches can move. \n",
    "\n",
    "Here you can specify any number of striding window or patch size but make sure if you give very small it leads to more patches then it is `computationaly expensive` method to train the model. \n",
    "\n",
    "<center><img src=\"images/over.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060037f1-6507-4c61-b107-112190c57be0",
   "metadata": {},
   "source": [
    "Now, we break the images into different patches. Now we need to convert our patch matrix to vector for processing. This process is also called `vectorization`\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"images/vec.png\" width=\"900\"/></center>\n",
    "\n",
    "\n",
    "### 2. Indepth architecture: \n",
    "\n",
    "It is basically a **`encoder`** based architecture. Just only a original encoder block. \n",
    "\n",
    "Then we pass our images to the neural network with `positional encoding`. Positional encoding is very important for this task. You can use whatever positional encoding for this taks because all are giving the same results. \n",
    "\n",
    "\n",
    "<center><img src=\"images/pos.png\" width=\"600\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade025c6-70ab-43bd-8191-f8c6cba3b8ab",
   "metadata": {},
   "source": [
    "Then we add `[CLS]` token to the input vectors. Because this is a classification transformer. \n",
    "\n",
    "<center><img src=\"images/clss.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e1a5e-abdf-4bfe-b58b-7f59ea1b480a",
   "metadata": {},
   "source": [
    "Then we add `Multi head self attetion` layer -> `dense layer` -> `multi head self attention` -> `dense layer`. \n",
    "\n",
    "\n",
    "<center><img src=\"images/head.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0be92f-bc8c-4d0a-a5d8-3d13d38316a5",
   "metadata": {},
   "source": [
    "**`Full Encoder block`** \n",
    "\n",
    "<center><img src=\"images/half.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8348c83-889d-4eb3-b31b-f0b24fe3f8d0",
   "metadata": {},
   "source": [
    "**`Full Transformer Block`** \n",
    "\n",
    "<center><img src=\"images/full.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e1f58-3628-4c6d-b475-3fc52dc36ba7",
   "metadata": {},
   "source": [
    "Here we got the output `co`, `c1`, `c2` ... `cn`  \n",
    "\n",
    "We already studied that **`Co`** is the aggregation of all the remaining output in the BERT architecuture (encoder architecture), So we take only the `Co` and give it to the softmax layer. \n",
    "\n",
    "<center><img src=\"images/co.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8c749-40ca-4f4c-9402-61968278bda6",
   "metadata": {},
   "source": [
    "Then we get the probabilites of each output~ \n",
    "\n",
    "<center><img src=\"images/final.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88018a4-8a0e-48f7-8aa2-ca4750b1392e",
   "metadata": {},
   "source": [
    "Here the orignial architecutre from original paper! \n",
    "\n",
    "<center><img src=\"images/vision.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9766b-b48a-4ca3-85e3-9496a50f3846",
   "metadata": {},
   "source": [
    "Variants of `VIT` \n",
    "\n",
    "<center><img src=\"images/variants.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5889344-066e-442d-8269-12513e972982",
   "metadata": {},
   "source": [
    "Experiments~ \n",
    "\n",
    "<center><img src=\"images/experiments.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab57eb2-6b14-4c8e-9670-c6f5df13f360",
   "metadata": {},
   "source": [
    "`Take away point`: It's learns exactly same as the `CNNs` the difference is that here we don't use any cnn operations, we use only transformer operations so we get high attention on `each vectors` but in normal CNNs we don't get any attention, This is the main reason it outperforms the normal CNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd293bef-0123-4452-8da7-8e5e4b781b76",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "\n",
    "1. [**Shusen Wang**](https://www.youtube.com/watch?v=HZ4j_U3FC94) \n",
    "2. [**Yannic Kilcher**](https://www.youtube.com/watch?v=TrdevFK_am4&t=43s) \n",
    "3. [**Slides**](https://github.com/wangshusen/DeepLearning/blob/master/Slides/10_ViT.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
