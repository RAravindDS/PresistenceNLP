{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049d2d50-ecd7-451f-b8c2-412f1be29839",
   "metadata": {},
   "source": [
    "<h1><center>ğ“ğ“½ğ“½ğ“®ğ“·ğ“½ğ“²ğ“¸ğ“· ğ“¶ğ“¸ğ“­ğ“®ğ“µğ“¼ ğŸ‰</center></h1> \n",
    "\n",
    "[**Reasearch_Paper**](https://arxiv.org/pdf/1409.0473.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08ddf9-e21c-4306-af68-485230e00dda",
   "metadata": {},
   "source": [
    "This is different from encoder and decoder, in this attention algorithm we don't have any encoder and decoder here and it is better than encoder and decoder architecutre. \n",
    "\n",
    "The problem in the encoder and decoder if the sentence lenght increases the accuracy of the model(blue score) decreases. \n",
    "\n",
    "<img src=\"images/blue.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Why the sentence increaes the accuracy decreases? because we are taking output from the end of the encoder, the output is the vector that represent all the input sentence but in our case when the sentence length increases the vector(output from the encoder) can't reprsent all the words, that'w why we are getting low accuracy from the model translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503b8c5-4382-47c5-a061-bd081e627315",
   "metadata": {},
   "source": [
    "The main differnce is that, when you are using the enocder and decoder model (the word tranlsation would be memorizing all the input sentences and finding the high probl output sentences) here the attention model willl do like human (it will take first few set of words and translate and it will take second few set of words and translate), so we get rid away of any searching algorithm. \n",
    "\n",
    "\n",
    "<img src=\"images/att.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3f455-3f72-405b-b485-19131c2083bf",
   "metadata": {},
   "source": [
    "The main aim of the attention model is that it will give attention to each and specific word(if you are tranlsating first word of the input, it will give more attention to the first word, if you are tranlsating second word then it will give more attention to the second word, like wise .. n), this is what exactly attention model doing now. The output calculated by the all the outputs of the rnn based on some context, see the top neural network to understand this line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5c473-0ba9-44e9-9597-129ba070c0d8",
   "metadata": {},
   "source": [
    "How it giving the attention to the first word while translating first word, it is because of the alpha funciton formula(this function uses the small neural network, so it automatically learn the parameters from the inputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92037606-fbf9-440e-8641-980941253a8b",
   "metadata": {},
   "source": [
    "you can see the alpha values that present between any blstm layer, that helps to give more attention to the specific words. \n",
    "\n",
    "<img src=\"images/fu.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7704a3-4b16-4878-be71-24000b21a081",
   "metadata": {},
   "source": [
    "**The key difference is that, encoder and decoder rerpesent all the input words in the single vector, but the attention model uses the sequence of vector to represent the input text so, we will get more accuracy compare to the encoder and decoder.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0d344-eb8c-4124-b20a-4032628bfadf",
   "metadata": {},
   "source": [
    "This is good why we need transformer, because as a sentence length increase, the accuracy got decrease. So, we are moving to the transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ae96a-a3d0-49fa-a673-86935a6af6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
