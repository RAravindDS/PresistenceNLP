{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ac3210-6238-4d23-b7b1-55d80d3c04af",
   "metadata": {},
   "source": [
    "<center><h1>ğ“¡ğ“¸ğ“«ğ“¾ğ“¼ğ“½ğ“µğ”‚ ğ“ğ“¹ğ“½ğ“²ğ“¶ğ“²ğ”ƒğ“®ğ“­ ğ“‘ğ“”ğ“¡ğ“£ ğ“Ÿğ“»ğ“®-ğ“½ğ“»ğ“ªğ“²ğ“·ğ“²ğ“·ğ“° ğ“ğ“¹ğ“¹ğ“»ğ“¸ğ“ªğ“¬ğ“±</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f616b-c230-4940-9a47-5c4a0a474132",
   "metadata": {},
   "source": [
    "In this chapter, we will build a RoBERTa model from scratch. The model will take \n",
    "the bricks of the Transformer construction kit we need for BERT models. Also, no \n",
    "pretrained tokenizers or models will be used. The RoBERTa model will be built \n",
    "following the fifteen-step process described in this chapter.\n",
    "\n",
    "In **first chapter** we understood **building blocks of transformer** then in 2nd chapter** we fine tuned a **BERT** model, in **this chapter** we will build a **pretrained transformer** from **scracth** using a **hugging face** lib with the transformer name called **KantaiBERT**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f321c-5c14-4bf4-82cf-51e99e1c78bb",
   "metadata": {},
   "source": [
    "KantaiBERT trains its own tokenizer from scratch. It will build its merge and \n",
    "vocabulary files, which will be used during the pretraining process.\n",
    "KantaiBERT then processes the dataset, initializes a trainer, and trains the model.\n",
    "\n",
    "Finally, KantaiBERT uses the trained model to perform an experimental downstream \n",
    "language modeling task and fills a mask using Immanuel Kant's logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbcaa2-b2be-48e3-896e-0a09ed47e47d",
   "metadata": {},
   "source": [
    "## Building KantaiBert Transformer from Scratch \n",
    "\n",
    "**KantaiBERT** is a **R**obustly **O**ptimized **B**ERT **P**retraining **A**pproach **(RoBERTa)**-like \n",
    "model based on the architecture of BERT.\n",
    "\n",
    "The initial BERT models were undertrained. RoBERTa increases the performance of the pretrained transformers for downstream tasks.  RoBERTa has improved the mechanics of the pretraining process. For example, it **does not use WordPiece** tokenization but goes down to **byte-level Byte Pair Encoding (BPE)**\n",
    "\n",
    "This KantaiBert we are going to build is a small model 6 layers, 12 heads, and 84,095,008 parameters. (84million) \n",
    "KantaiBERT will use a custom dataset, train a tokenizer, train the transformer model, \n",
    "save it, and run it with a masked language modeling example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aaabfa-33e0-478f-aebe-0f0dfd824957",
   "metadata": {},
   "source": [
    "[**OurCode**](https://colab.research.google.com/drive/17BQEm9wXb8fvTGCex9PRoVsGqBx0WEAi#scrollTo=qR4a49VNhr3n)\n",
    "\n",
    "[**OfficicalCode**](https://github.com/PacktPublishing/Transformers-for-Natural-Language-Processing/blob/main/Chapter03/KantaiBERT.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
