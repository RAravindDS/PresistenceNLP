{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12f0511-6a00-4ed0-b785-7ed95dbe9f80",
   "metadata": {},
   "source": [
    "<center><h1>\n",
    "    \n",
    "    ùîæùïñùï•ùï•ùïöùïüùïò ùïäùï•ùïíùï£ùï•ùïñùïï ùï®ùïöùï•ùïô \n",
    "    ùï•ùïôùïñ ùïÑùï†ùïïùïñùïù ùî∏ùï£ùïîùïôùïöùï•ùïñùïîùï•ùï¶ùï£ùïñ \n",
    "    ùï†ùïó ùï•ùïôùïñ ùïãùï£ùïíùïüùï§ùïóùï†ùï£ùïûùïñùï£\n",
    "</h1></center>    \n",
    "    \n",
    "[**Codes**](https://github.com/PacktPublishing/Transformers-for-Natural-Language-Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bb7ae-8827-4048-9284-6ee4397a1be2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In December 2017, the seminal Vaswani et al. Attention Is All You Need article, written \n",
    "by Google Brain members and Google Research, was published. The Transformer \n",
    "was born. The Transformer outperformed the existing state-of-the-art NLP models. \n",
    "The Transformer trained faster than previous architectures and obtained higher \n",
    "evaluation results. Transformers have become a key component of NLP.\n",
    "\n",
    "Before this innovation, there are lot's of innovation from early's ninties, There are lot's of problem with the old network like, long short term memory. To overcome this the **Transformer came**. The main important point is **This is parallel computing** compare to old RNN, CNN, BLSTM, this is a parallel computing. \n",
    "\n",
    "Here there is no RNN, CNN, LSTM, BRNN, BLSTM, here the new concept let's break it ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dff32-f28d-4f2a-a736-ba7385a65e29",
   "metadata": {},
   "source": [
    "<center><img src=\"https://quantdare.com/wp-content/uploads/2021/11/transformer_arch.png\" width=\"400\" height=\"200\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534a8de-ab4f-4cf2-a204-7ec32711e11a",
   "metadata": {},
   "source": [
    "There is a 6-layer encoder stack \n",
    "on the left and a 6-layer decoder stack on the right. On the left, the inputs enter the encoder side of the Transformer through an attention \n",
    "sub-layer and FeedForward Network (FFN) sub-layer. On the right, the target \n",
    "outputs go into the decoder side of the Transformer through two attention sub-layers \n",
    "and an FFN sub-layer.\n",
    "\n",
    "The attention mechanism \n",
    "is a **\"word-to-word\"** operation. The attention mechanism will find how **each word \n",
    "is related to all other words in a sequence**, including the word being analyzed itself..\n",
    "\n",
    "Let's see some example: \n",
    "\n",
    "```\n",
    "Example word: \n",
    "The cat sat on the mat.\n",
    "```\n",
    "\n",
    "<img src=\"images/att.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Attention will run dot products between word vectors and determine the strongest \n",
    "relationships of a word among all the other words, including itself (\"cat\" and \"cat\"):, \n",
    "\n",
    "NOte: **Here we are not running single attention system, we are parallely running 8 attention systems**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82793152-214e-4ee3-bc42-820faff89cd1",
   "metadata": {},
   "source": [
    "## The Encoder Stack\n",
    "\n",
    "\n",
    "The layers in the tranformers are **stack of layers**. Let's see the encoder architecture now! \n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505e963-7699-44da-b09b-b663e0440ebc",
   "metadata": {},
   "source": [
    "Each layer contains two main sub-layers: a **multi-headed \n",
    "attention mechanism** and a **fully connected position-wise feedforward network.**\n",
    "\n",
    "These connections transport the unprocessed input **x** of a **sublayer** to a **layer normalization** function. This way, we are certain that key information \n",
    "such as **positional encoding is not lost on the way**. The normalized output of each \n",
    "layer is thus:\n",
    "\n",
    "```\n",
    "LayerNormalization (x + Sublayer(x))\n",
    "```\n",
    "\n",
    "\n",
    "The designers of the Transformer introduced a very efficient constraint. The output \n",
    "of every sub-layer of the model has a constant dimension, including the embedding \n",
    "layer and the residual connections. This dimension is dmodel and can be set to another \n",
    "value depending on your goals. In the original Transformer architecture, d(model) =512.\n",
    "d(model) has a powerful consequence. \n",
    "\n",
    "Practically all the key operations are dot products. \n",
    "The dimensions remain stable, which reduces the number of operations to calculate, \n",
    "reduces machine consumption, and makes it easier to trace the information as it \n",
    "flows through the model\n",
    "\n",
    "Let's go from very scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676f3da-3d4d-41ed-8c72-a7eff76bf755",
   "metadata": {},
   "source": [
    "### The inputs Embedding \n",
    "\n",
    "The input embedding sub-layer converts the input tokens to vectors of dimension \n",
    "d = 512 using learned embeddings in the original Transformer model. The \n",
    "structure of the input embedding is classical:\n",
    "\n",
    "<center><img src=\"images/input.png\" width=\"500\"/></center>\n",
    "\n",
    "The embedding sub-layer works like other standard transduction models. A \n",
    "tokenizer will transform a sentence into tokens. Each tokenizer has its methods, \n",
    "but the results are similar. For example, a tokenizer applied to the sequence \n",
    "```\"the \n",
    "Transformer is an innovative NLP model!\" \n",
    "```\n",
    "will produce the following tokens in \n",
    "one type of model\n",
    "\n",
    "```Python \n",
    "['the', 'transform', 'er', 'is', 'a', 'revolutionary', 'n', 'l', 'p', 'model', '!']\n",
    "```\n",
    "You will notice that this tokenizer normalized the string to lower case and truncated \n",
    "it into subparts. A tokenizer will generally provide an integer representation that will \n",
    "be used for the embedding process. For example:\n",
    "\n",
    "```Python \n",
    "Text = \"The cat slept on the couch.It was too tired to get up.\"\n",
    "tokenized text= [1996, 4937, 7771, 2006, 1996, 6411, 1012, 2009, 2001, 2205, 5458, 2000, 2131, 2039, 1012]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1df71-43c0-4d6a-bd71-f0cea293a28d",
   "metadata": {},
   "source": [
    "There is no other way to be tokenize this sentence, so it need to be embedded, The Transformer contains a learned embedding sub-layer. The learned embedding sublayer should be given by us (I mean we need to embedding the sentence to same size), in the transformer they used `512` dimensions, you can use whatever you want.  You can use `word2vec` or `Glove` whatever you want, but this needs to be in the same dimensions. \n",
    "\n",
    "However, a big chunk of information is missing because no additional vector or information indicates a word's position in a sequence. (I mean which word came whic area) EX: \n",
    "\n",
    "```\n",
    "MY name is aravind\n",
    "```\n",
    "\n",
    "We need to specify the position of the word `my` as `1`, `name` as `2` like that. If we do this we don't loss the `context` of the sentence. \n",
    "\n",
    "### Positional Encoding: \n",
    "\n",
    "<center><img src=\"images/pos.png\" width=\"500\"/></center>\n",
    "\n",
    "We will pass **high dimension** vector to the **input embedding**, the vector space should be very high. So, it's tough to find which word is starting of the sentence, which word is middle of the sentence like that, instead of having additional vectors to describe the position of the vectors. The idea is to add **`positional encoding`** value to the input embedding. \n",
    "\n",
    "\n",
    "If we go back to the sentence we used in the word embedding sub-layer, we can see \n",
    "that black and brown may be similar, but they are far apart:\n",
    "\n",
    "Example: \n",
    "**`The black cat sat on the couch and the brown dog slept on the rug.`**\n",
    "\n",
    "The word black is in position 2, pos=2, and the word brown is in position 10, pos=10. \n",
    "\n",
    "Our problem is to find a way to add a value to the word embedding of each word \n",
    "so that it has that information. However, we need to add a value to the dmodel = 512 \n",
    "dimensions! For each word embedding vector, we need to find a way to provide \n",
    "information to i in the range(0,512) dimensions of the word embedding vector of \n",
    "black and brown. \n",
    "\n",
    "There are many ways to do this, one of the clever way is that unit sphere to represent positional encoding with sine and cosine values. \n",
    "\n",
    "<center><img src=\"images/fom.png\" width=\"500\"/></center>\n",
    "\n",
    "If we start at the beginning of the word embedding vector, we will begin with a \n",
    "constant (512), i=0, and end with i=511. This means that the sine function will be \n",
    "applied to the even numbers and the cosine function to the odd numbers. Some \n",
    "implementations do it differently. In that case, the domain of the sine function can be \n",
    "ùëñùëñ ‚àà [0,255] and the domain of the cosine function can be ùëñùëñ ‚àà [256,512]. This will produce \n",
    "similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc7d39-1049-499d-8b94-81a539c95ee5",
   "metadata": {},
   "source": [
    "\n",
    "```Python\n",
    "# Positional encoding: \n",
    "\n",
    "import math \n",
    "def positional_encoding(pos, pe): \n",
    "    \n",
    "    for i in range(0, 512, 12): \n",
    "        pe[0][i] = math.sin(pos / (10000* ((2 * i) / d_model)) ) \n",
    "        pe[0][i+1] = math.cos(pos / (10000 ** ((2 * i)/d_model)) ) \n",
    "    return pe \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0c740-1d01-434c-92e5-4240687fedb7",
   "metadata": {},
   "source": [
    "If we go back to the sentence we are parsing in this section, we can see that black is \n",
    "in position pos=2 and brown is in position pos=10:\n",
    "\n",
    "`\n",
    "The black cat sat on the couch and the brown dog slept on the rug.`\n",
    "\n",
    "If we apply the sine and cosine functions literally for pos=2, we obtain a size=512\n",
    "positional encoding vector:\n",
    "\n",
    "<center><img src=\"images/pos_vec.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2b77b-b20b-41d1-8036-f95e1d994750",
   "metadata": {},
   "source": [
    "The cosine similarity function used for word embedding comes in handy for having \n",
    "a better visualization of the proximity of the positions:\n",
    "\n",
    "`cosine_similarity(pos(2), pos(10)= [[0.8600013]]` # pos 2 -> black, pos 3 -> brown \n",
    "\n",
    "The similarity between the position of the words black and brown and the lexical \n",
    "field (groups of words that go together) similarity is different:\n",
    "\n",
    "`cosine_similarity(black, brown)= [[0.9998901]]`\n",
    "\n",
    "The encoding of the position shows a lower similarity value than the word \n",
    "embedding similarity, In simple words, the cosine similarity of the normal word `black and brown` is high, whereas the positonal encoding of the same word is less, this is due to positional encoding operations. \n",
    "\n",
    "To solve this, we will use this simple formula: \n",
    "\n",
    "<center><img src=\"images/podu.png\" width=\"500\"/></center>\n",
    "\n",
    "**`pc(black)=y1+pe(2)`**\n",
    "\n",
    "The solution is straightforward. However, if we apply it as shown, we might lose \n",
    "the information of the word embedding, which will be minimized by the positional \n",
    "encoding vector.\n",
    "\n",
    "There are many possibilities to increase the value of y1 to make sure that the \n",
    "information of the word embedding layer can be used efficiently in the subsequent \n",
    "layers.\n",
    "\n",
    "One of the many possibilities is to add an arbitrary value to y1, the word embedding \n",
    "of black:\n",
    "\n",
    "**`y1*math.sqrt(dmodel)`** \n",
    "\n",
    "If we apply this to the vectors, we will get the cosine similarity like\n",
    "\n",
    "<center><img src=\"images/final.png\" width=\"800\"/></center>\n",
    "\n",
    "**`This output will go the multi-head attention sub-layer`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb104bbc-845b-40c0-806c-32e9293cfd92",
   "metadata": {},
   "source": [
    "### SubLayer1 MultiHeadAttention\n",
    "\n",
    "The multi-head attention sub-layer contains **eight heads** and is followed by post **layer normalization**, which will add residual connections to the output of the sublayer and normalize it:\n",
    "\n",
    "<center><img src=\"images/sub.png\" width=\"500\"/></center>\n",
    "\n",
    "The input of the multi-attention sub-layer of the first layer of the encoder stack is a \n",
    "vector that contains the embedding and the positional encoding of each word.\n",
    "\n",
    "vector = **`pe(xn)=[d1 =9.09297407e-01, d2=9.09297407e-01,.., d512 = 1.00000000e+00]`**\n",
    "\n",
    "`Sentence = The cat sat on the rug and it was dry cleaned`\n",
    "\n",
    "The model will train to find out if \"it\" is related to \"cat\" or \"rug.\" We could run a \n",
    "huge calculation by training the model using the dmodel = 512 dimensions as they are \n",
    "now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb89dc-2622-4013-8159-590d1677f5ad",
   "metadata": {},
   "source": [
    "We are running 8 heads, if we run all the sentence in a one head, it will take time, so we are going to divide by 8. why eight, if you want to run parallely (we have 8 heads) 512//3 = 64, so we can give 64 vector to each head for parallel computing. \n",
    "\n",
    "<center><img src=\"images/8.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9d712-a035-473e-9e7f-441fb804b02f",
   "metadata": {},
   "source": [
    "The each head determies the `it` refers to cat or rug. The output of this is a matrix z = \n",
    "\n",
    "**`Z = (z0\n",
    ", z1\n",
    ", z2\n",
    ", z3\n",
    ", z4\n",
    ", z5\n",
    ", z6\n",
    ", z7\n",
    ",)\n",
    "`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee03654-f3d4-443a-bc80-ddd2765b35c2",
   "metadata": {},
   "source": [
    "NOw the **Z** is the sequence of output, we need a single output, so we will concatenate this \n",
    "\n",
    "**`MultiHead(output) = Concat(z0, z1, z2, z3, z4, z5, z6, z7,) = x, dmodel`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f29cca-62ba-4696-b6d4-9e9cccc6762a",
   "metadata": {},
   "source": [
    "Let's see what's inside each head. In each head(vector) has three representation: \n",
    "\n",
    "* Querey Vector\n",
    "* Key Vector \n",
    "* Value Vector \n",
    "---\n",
    "* A query vector (Q) that has a dimension of dq = 64, which is activated and  trained when a word vector xn  seeks all of the key-value pairs of the other  word vectors, including itself in self-attention\n",
    "* A key vector (K) that has a dimension of dk = 64, which will be trained to provide an attention value\n",
    "* A value vector (V) that has a dimension of dv= 64, which will be trained to  provide another attention value\n",
    "\n",
    "Attention is defined as \"Scaled Dot-Product Attention,\" which is represented in the \n",
    "following equation in which we plug Q, K, and V:\n",
    "\n",
    "<center><img src=\"images/attention.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c3882-286b-4ab3-b998-c35c5e9a14de",
   "metadata": {},
   "source": [
    "The vectors all have the same dimension making it relatively simple to use a scaled \n",
    "dot product to obtain the attention values for each head and then concatenate the \n",
    "output Z of the 8 heads.\n",
    "\n",
    "To obtain Q, K, and V, we must train the model with their respective weight matrices \n",
    "Qw, Kw and Vw, which have dk\n",
    " = 64 columns and dmodel = 512 rows. For example, Q is \n",
    "obtained by a dot-product between x and Qw. Q will have a dimension of dk\n",
    " = 64.\n",
    " \n",
    "This things done internally in the hugging face transformers, Trax and more.. Let's see inside what happening in 10 steps: \n",
    "\n",
    "#### Step1: \n",
    "\n",
    "The input of the attention mechanism we are building is scaled down to dmodel = 4 \n",
    "instead of dmodel = 512. This brings the dimensions of the vector of an input x down to \n",
    "dmodel = 4, which is easier to visualize. \n",
    "\n",
    "x contains 3 inputs with 4 dimensions each instead of 512:\n",
    "\n",
    "ok fine, See the code [**here**](https://github.com/PacktPublishing/Transformers-for-Natural-Language-Processing/blob/main/Chapter01/Multi_Head_Attention_Sub_Layer.ipynb) from now \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bced52-8d78-47f5-825b-2edcd98c3a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy.special import softmax\n",
    "# step 1: \n",
    "''' Three inputs with the dimension of 4 '''\n",
    "\n",
    "x = np.array([ [1.0, 0.0, 1.0, 0.0],  # input 1 \n",
    "               [0.0, 2.0, 0.0, 2.0],  # input 2\n",
    "               [1.0, 1.0, 1.0, 1.0]   # input 3 (consider this is the id from the respective word of vocabulary \n",
    "]) \n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90788f92-179b-4d92-a611-3d739c06a852",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "<center><img src=\"images/step1.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f771c15-1085-470d-93d0-1ab26d950cc2",
   "metadata": {},
   "source": [
    "#### Step2: (Initializing the weights) \n",
    "\n",
    "Each input has **3 weights**. \n",
    "* Qw to train the queries. \n",
    "* Kw to train the keys. \n",
    "* Vw to train the values. \n",
    "\n",
    "IN the transformer paper, we have 64 dimension weights for 512 dimension. similarly we are going to have 3 dimesnsion of the vector. Then we can perform the dot product between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b3cc05-8ff6-45b4-a993-d99759329223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: (weight initialization) \n",
    "\n",
    "w_query =np.array([ [1, 0, 1],   # random initializaiton\n",
    "                    [1, 0, 0],\n",
    "                    [0, 0, 1],\n",
    "                    [0, 1, 1] ])  # dimension is 4 X 3 because the input dimesion is 3 X 4 \n",
    "\n",
    "w_key =np.array([ [0, 0, 1],\n",
    "                  [1, 1, 0],\n",
    "                  [0, 1, 0],\n",
    "                  [1, 1, 0] ])\n",
    "\n",
    "w_value = np.array([ [0, 2, 0],\n",
    "                     [0, 3, 0],\n",
    "                     [1, 0, 3],\n",
    "                     [1, 1, 0] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b82ec0-e559-42ed-9403-b385d188fa3a",
   "metadata": {},
   "source": [
    "<center><img src=\"images/step2.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a7df0-9459-46fb-b586-1b41aa4526eb",
   "metadata": {},
   "source": [
    "#### Step3 (Matric Multi to Obtain Q,K,V): \n",
    "\n",
    "We will now multiply the input vectors by the weight matrices to obtain a query, \n",
    "key, and value vector for each input.\n",
    "\n",
    "In this model, we will assume that there is one w_query, w_key, and w_value weight \n",
    "matrix for all inputs. Other approaches are possible. In transformers paper, we will do like 8 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c35d580b-a171-4264-ad0b-97b4dc893a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# step3 (matrix multi to obtain Q, K, V)\n",
    "\n",
    "Q = np.matmul(x, w_query)  # input x multiply with weight matrix q \n",
    "K = np.matmul(x, w_key) \n",
    "V = np.matmul(x, w_value) \n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144609f3-f801-472a-9b83-ea4c5c918c88",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "<center><img src=\"images/step3.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2802c1-b311-4402-a6cf-1cd10774d990",
   "metadata": {},
   "source": [
    "#### Step4 (Scaled Attention Scores): \n",
    "\n",
    "Now, we are going to do the attention formula, we discussed earlier: \n",
    "<center><img src=\"images/attention.png\" width=\"300\"/></center>\n",
    "\n",
    "Here in thi step we will focus on Q and K \n",
    "\n",
    "For this model, we will round ‚àöùëëùëò = ‚àö3 = 1.75 to 1 and plug the values into the Q\n",
    "and K part of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a717d0f7-7e30-4b7d-9a5d-e5481942051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  4.],\n",
       "       [ 4., 16., 12.],\n",
       "       [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step4 (Q and K) implementation \n",
    "\n",
    "k_d = 1   # square root of 3 is 1.75 to 1\n",
    "\n",
    "attention_scores = ( Q @ K.transpose() )/ k_d \n",
    "\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29152e21-4cc1-40de-b864-c7114c25d00b",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "\n",
    "<center><img src=\"images/step4.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27857b-44f6-42d5-b0f9-dd033c51effb",
   "metadata": {},
   "source": [
    "#### Step5 (Scaled softmax of attention output)\n",
    "\n",
    "We now apply a softmax function to each intermediate attention score. Instead of \n",
    "doing a matrix multiplication, let's zoom down to each individual vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf92b865-cdad-4b2f-b83d-af9e3e49369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25010005 0.37494998 0.37494998]\n",
      " [0.2133134  0.56950313 0.21718348]\n",
      " [0.22037557 0.53143172 0.24819272]]\n"
     ]
    }
   ],
   "source": [
    "# step5 Scaling the attention outputs using softmax \n",
    "\n",
    "attention_scores[0] = softmax(attention_scores[0])  # taking first word and calculating attention \n",
    "attention_scores[1] = softmax(attention_scores[1]) \n",
    "attention_scores[2] = softmax(attention_scores[2])  # taking third word and calculating attention score... \n",
    "\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d90b6-5448-403e-b8f5-2bb1a0de2cc4",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "\n",
    "<center><img src=\"images/step5.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1f5bc-29c1-4340-9b68-1f96ed654b69",
   "metadata": {},
   "source": [
    "#### Step6 (The final calculation of the attention formula) \n",
    "\n",
    "we have missed the v value there, let's plug in here.. \n",
    "\n",
    "To obtain Attention (Q,K,V) for x1\n",
    ", we multiply the intermediate attention score by the \n",
    "3 value vectors one by one to zoom down into the inner workings of the equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31f47f13-fcc2-4bff-919b-c145dd884c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25010005 0.5002001  0.75030014]\n",
      "[0.74989995 2.99959981 0.        ]\n",
      "[0.74989995 2.24969986 1.12484993]\n"
     ]
    }
   ],
   "source": [
    "# Step6 finising the function:\n",
    "\n",
    "attention1 = attention_scores[0].reshape(-1,1) \n",
    "attention1 = attention_scores[0][0]*V[0]   # multiplying the input vector for 1st word [1,2,3] with v:: [1].[v[1]] [2].[v[2]] [3].[v[3]]\n",
    "\n",
    "attention2 = attention_scores[0][1]*V[1]\n",
    "\n",
    "attention3 = attention_scores[0][2]*V[2]\n",
    "\n",
    "print(attention1)\n",
    "print(attention2)\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c2498cd-9e11-4f5d-8a61-f6b6b02c2eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4683105308334813"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1333a3c-9b56-4454-a832-9b73e1d61590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 6., 3.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d65c55b-64dc-4b84-813c-5679ee101333",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "\n",
    "<center><img src=\"images/step6.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c95c7a-97c6-42ca-8d30-f0d12ce03d7f",
   "metadata": {},
   "source": [
    "#### Step7 (summing all) \n",
    "\n",
    "The 3 attention values of input #1 obtained will now be summed to obtain the first \n",
    "line of the output matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd9518b5-1f2c-4813-a8e0-d67f78dd1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "# step7 summing all the first input\n",
    "attention_input1 = attention1 + attention2 + attention3\n",
    "\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09793a1f-925b-446d-a212-53a59a4fec04",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "\n",
    "<center><img src=\"images/step7.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b292e7-bb7d-49f8-b206-a7ba2395abbf",
   "metadata": {},
   "source": [
    "This is how, we need to calculate the attention score for each input, let's do this for all the inputs: \n",
    "\n",
    "#### Step8 (Step1 to 7 for all the inputs): \n",
    "\n",
    "The Transformer can now produce the attention values of input #2 and input #3 \n",
    "using the same method described from Step 1 to Step 7 for one attention head.\n",
    "\n",
    "We have seen the attention representation process in detail with a small model. \n",
    "Let's go directly to the result and assume we have generated the 3 attention \n",
    "representations with a dimension of dmodel = 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d418a46-706d-46ed-9cdf-8c5c3320774d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.10280484 0.39598973 0.45517602 0.58647984 0.58828552 0.92122988\n",
      "  0.68654625 0.80763479 0.20589559 0.72121693 0.29703272 0.17667301\n",
      "  0.27364887 0.17253768 0.90346001 0.90633138 0.7933515  0.51964885\n",
      "  0.80541971 0.50531064 0.25783709 0.82806367 0.96314343 0.4034734\n",
      "  0.79432498 0.46931542 0.19238388 0.83910951 0.04350163 0.94790374\n",
      "  0.20440588 0.30288193 0.40085491 0.55028486 0.30983402 0.31681075\n",
      "  0.50154442 0.00679641 0.60881248 0.10793141 0.66959068 0.53781646\n",
      "  0.52632209 0.56625682 0.60856186 0.70206512 0.6267506  0.55595726\n",
      "  0.50485428 0.08930771 0.9205643  0.04438278 0.67530187 0.81699594\n",
      "  0.92107407 0.17636544 0.22964583 0.35607933 0.50555196 0.8029805\n",
      "  0.92139223 0.22856835 0.046276   0.99414017]\n",
      " [0.21641989 0.8765781  0.77964355 0.60833895 0.99334575 0.53972279\n",
      "  0.05616948 0.67679237 0.86523839 0.79496068 0.59690904 0.33907428\n",
      "  0.11532851 0.04131901 0.67542439 0.05019122 0.0950849  0.56793785\n",
      "  0.02515496 0.03564373 0.47345623 0.76671431 0.72061418 0.02951797\n",
      "  0.56671611 0.93615503 0.37875437 0.56512249 0.24263582 0.85265506\n",
      "  0.66235199 0.90254092 0.50773426 0.91428733 0.96884329 0.20285123\n",
      "  0.16132222 0.00892091 0.78542053 0.98956031 0.19200163 0.01556933\n",
      "  0.07555919 0.42496279 0.58393617 0.54573323 0.88429065 0.40971545\n",
      "  0.85635989 0.15530371 0.55254906 0.24162646 0.2471039  0.10127338\n",
      "  0.97067432 0.49757969 0.25072509 0.30306125 0.57464374 0.84531659\n",
      "  0.54957953 0.71502235 0.00492932 0.04311346]\n",
      " [0.35146214 0.06879758 0.92354148 0.54889105 0.77475482 0.72443994\n",
      "  0.69906245 0.27346078 0.34724256 0.64563736 0.48112669 0.44730361\n",
      "  0.00648112 0.21658372 0.13505376 0.35531105 0.99120867 0.47924454\n",
      "  0.6255888  0.56650256 0.87786666 0.82987137 0.20025252 0.82346326\n",
      "  0.57588546 0.35053714 0.87531089 0.91553053 0.29547604 0.53803275\n",
      "  0.69977309 0.06237829 0.87908085 0.07872017 0.90063374 0.28309945\n",
      "  0.2752983  0.36129441 0.52062932 0.88349722 0.41263036 0.87601732\n",
      "  0.72980748 0.61525077 0.68575565 0.82023412 0.92338122 0.00692434\n",
      "  0.8168606  0.79515524 0.89702565 0.62748625 0.81962772 0.15232284\n",
      "  0.21486763 0.5104639  0.70729471 0.48709236 0.48092275 0.01145111\n",
      "  0.67690344 0.29178762 0.62917108 0.30733706]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "\n",
    "#We assume we have 3 results with learned weights (they were not trained in this example)\n",
    "#We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
    "\n",
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)   # original output of transformer paper like.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525137b-3a5f-4248-af12-e6b808f894ce",
   "metadata": {},
   "source": [
    "#### Step9 (The output of the heads of attention sub layer): \n",
    "\n",
    "We assume that we have trained the 8 heads of the attention sub-layer. The \n",
    "transformer now has 3 output vectors (of the 3 input vectors that are words or word \n",
    "pieces) of dmodel = 64 dimensions each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b82fe4-dd0c-4bf0-98e8-40736343a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64)\n"
     ]
    }
   ],
   "source": [
    "# step 9\n",
    "\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "\n",
    "print(z0h1.shape)  # 3 input with 64 dimension (with all 8 heads) *understand? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b884f86-284e-4aac-9405-5a2117d88cfd",
   "metadata": {},
   "source": [
    "#### Step 10: Concatenation of the output of the heads\n",
    "\n",
    "The Transformer concatenates the 8 elements of Z:\n",
    "\n",
    "**`\n",
    "MultiHead(output) = Concat(z0\n",
    ", z1\n",
    ", z2\n",
    ", z3\n",
    ", z4\n",
    ", z5\n",
    ", z6\n",
    ", z7\n",
    ",)W0\n",
    " = x, dmodel\n",
    " `**\n",
    " \n",
    "Note that Z is multiplied by W0\n",
    ", which is a weight matrix that is trained as well. \n",
    "In this model, we will assume W0\n",
    " is trained and integrated into the concatenation \n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af09dbb6-a6cc-4a26-a25d-2f3bdf6a10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0324842  0.54600819 0.93681112 ... 0.18769341 0.79759062 0.95800028]\n",
      " [0.84281932 0.66724598 0.07297926 ... 0.55198509 0.79932052 0.00509219]\n",
      " [0.15329525 0.54213658 0.48375129 ... 0.76061418 0.06414124 0.39099882]]\n"
     ]
    }
   ],
   "source": [
    "# step 10:  Concatenate all the heads we found: \n",
    "\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d7250b9-b6d5-4d59-a5c9-77d3ac21512a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_attention.shape   # inputs with 512 dimension and also attention outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7bc34-9f4b-450b-ad83-00f542f8c283",
   "metadata": {},
   "source": [
    "**`Visually`**\n",
    "\n",
    "<center><img src=\"images/step10.png\" width=\"500\"/></center>\n",
    "<center><img src=\"images/step7.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75afc739-7c82-4388-b898-79588906ad2c",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "The output from the multi head attention layer will flow to the **post layer normalization** and some of the positional encoded input also come directly to the layer normailzation. \n",
    "\n",
    "<center><img src=\"images/ps.png\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "It contain two process **add function** and **layer normalization process**. The add \n",
    "function processes the residual connections that come from the input of the sublayer. The goal of the residual connections is to make sure critical information is not \n",
    "lost. The Post-LN or layer normalization can thus be described as follows: \n",
    "\n",
    "**`LayerNorm = LayerNorm(x+Sublayer(x))`**\n",
    "\n",
    "Sublayer(x) is the sub-layer itself(input of the PLN). x is the information available at the input step of Sublayer(x).The input of LayerNorm is a vector v resulting from x + Sublayer(x). dmodel = 512 for \n",
    "every input and output of the Transformer, which standardizes all the processes.\n",
    "\n",
    "Many layer normalization methods exist, and variations exist from one model to \n",
    "another. The basic concept for v= x + Sublayer(x) can be defined by LayerNorm(v)\n",
    "\n",
    "\n",
    "**`Visually`**\n",
    "\n",
    "<center><img src=\"images/layernorm.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b4f20-dcc8-420b-a915-cc5f6129234f",
   "metadata": {},
   "source": [
    "**Why Layer Normalization and what are all the uses?** \n",
    "\n",
    "Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. \n",
    "\n",
    "Still doubt refer this [**research paper**](https://proceedings.neurips.cc/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a3d7f-c65e-4034-ba77-e7f35527fd65",
   "metadata": {},
   "source": [
    "## SubLayer2 FeedForward Neural Network\n",
    "\n",
    "The output of the LN is input to the FFN. \n",
    "\n",
    "<center><img src=\"images/ffn.png\" width=\"500\"/></center>\n",
    "\n",
    "It's just a normal neural network, it's fully connected FFN. This neural network is working on **Position wise**. \n",
    "\n",
    "**FFN containes two layers and apply the ReLU activation function**\n",
    "\n",
    "simple neural network \n",
    "\n",
    "**`FFN(x) = max(0, xW1 + b1 )W2 =b2`**\n",
    "\n",
    "The output of the FFN goes to the Post-LN, as described in the previous section. \n",
    "Then the output is sent to the next layer of the encoder stack and the multi-head \n",
    "attention layer of the decoder stack\n",
    "\n",
    "**`IGnore feature knowledge`** just for illustraion. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1304/0*esJuLjMbfGOhVnoJ.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354389a9-100c-49e8-9ea1-ac296370e7a9",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    " \n",
    "It's same like encoder, it also as a stack of layers. Let's see that \n",
    "\n",
    "<center><img src=\"images/decoder.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ffac7-1577-45ec-81cc-a5ae11476477",
   "metadata": {},
   "source": [
    "The structure of the decoder layer remains the same as the encoder for all the N=6\n",
    "layers of the Transformer model. Each layer contains three sub-layers: a multi headed masked attention mechanism, a multi-headed attention mechanism, and a \n",
    "fully connected position-wise feedforward network.\n",
    "\n",
    "### Multi head masked attention: \n",
    "\n",
    "The decoder has a third main sub-layer, which is the masked multi-head attention \n",
    "mechanism. In this sub-layer output, at a given position, the following words are \n",
    "masked so that the Transformer bases its assumptions on its inferences without \n",
    "seeing the rest of the sequence. That way, in this model, it cannot see future parts of \n",
    "the sequence. \n",
    "\n",
    "We need a method to prevent computing attention scores for future words. This method is called masking. To prevent the decoder from looking at future tokens, you apply a look ahead mask. The mask is added before calculating the softmax, and after scaling the scores. Let‚Äôs take a look at how this works.\n",
    "\n",
    "The mask is a matrix that‚Äôs the same size as the attention scores filled with values of 0‚Äôs and negative infinities. When you add the mask to the scaled attention scores, you get a matrix of the scores, with the top right triangle filled with negativity infinities.\n",
    "\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/0*QYFua-iIKp5jZLNT.png\" width=\"500\"/></center>\n",
    "\n",
    "In simple words, we mask future words, the encoder will predict (so it will learn the pattern of the data)\n",
    "\n",
    "The reason for the mask is because once you take the softmax of the masked scores, the negative infinities get zeroed out, leaving zero attention scores for future tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbde18-f9fd-4980-b310-a7fe68bc9f1b",
   "metadata": {},
   "source": [
    "### Output and Positional Encoding: \n",
    "\n",
    "The structure of the sub-layers of the decoder is mostly the same as the sub-layers \n",
    "of the encoder. The output embedding layer and position encoding function are the \n",
    "same as in the encoder stack.\n",
    "\n",
    "However, the masked multi-head attention sub-layer 1 only lets attention apply to \n",
    "the positions up to and including the current position. The future words are hidden \n",
    "from the Transformer, and this forces it to learn how to predict.\n",
    "\n",
    "\n",
    "#### attention \n",
    "\n",
    "The multi-head attention sub-layer 2 draws information from the encoder by taking \n",
    "encoder (K, V) into account during the dot-product attention operations. This sub\u0002layer also draws information from the masked multi-head attention sub-layer 1 \n",
    "(masked attention) by also taking sub-layer 1(Q) into account during the dot-product \n",
    "attention operations. The decoder thus uses the trained information of the encoder. \n",
    "We can define the input of the self-attention multi-head sub-layer of a decoder as:\n",
    "\n",
    "\n",
    "Input_Attention=(Output_decoder_sub_layer-1(Q), Output_encoder_layer(K,V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d41c14-6d12-45e9-bcf0-5fbf73ad59ef",
   "metadata": {},
   "source": [
    "### The FFN sub-layer, the Post-LN, and the linear layer\n",
    "\n",
    "Same as the encoder stack of layers. \n",
    "The Transformer produces an output sequence of only one element at a time:\n",
    "\n",
    "**`Output sequence= (y1, y2, ‚Ä¶ yn)`**\n",
    "\n",
    "The linear layer produces an output sequence with a linear function that varies per \n",
    "model but relies on the standard method:\n",
    "\n",
    "**`y = w*x + b`**\n",
    "\n",
    "x and b are learned parameters.\n",
    "\n",
    "The linear layer will thus produce the next probable elements of a sequence that a \n",
    "softmax function will convert into a probable element"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
