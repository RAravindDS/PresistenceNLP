{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fab70b-f398-444a-b3b5-21996f7aad1b",
   "metadata": {},
   "source": [
    "<center><h1>𝒜 𝐿𝐼𝒯𝐸 𝐵𝐸𝑅𝒯 𝐹𝒪𝑅 𝒮𝐸𝐿𝐹-𝒮𝒰𝒫𝐸𝑅𝒱𝐼𝒮𝐸𝒟\n",
    "𝐿𝐸𝒜𝑅𝒩𝐼𝒩𝒢 𝒪𝐹 𝐿𝒜𝒩𝒢𝒰𝒜𝒢𝐸 𝑅𝐸𝒫𝑅𝐸𝒮𝐸𝒩𝒯𝒜𝒯𝐼𝒪𝒩𝒮𝓢</h1></center>\n",
    "\n",
    "<center><h1> 9 FEB 2020 </h1></center>\n",
    "\n",
    "[**Reserach Paper**](https://arxiv.org/pdf/1909.11942.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a5349-376c-4ff2-aacd-dd3b7e2d32a6",
   "metadata": {},
   "source": [
    "Acutally this is same as BERT but this is ligher version of BERT, because bert large and medium size are very high and the got poor marks in benchmark dataset. We can't increase the number of parameters for accuracy. Instead of those, we use some new techniques in the ALBERT, that helps to get the good accuracy in ligher mode of BERT. \n",
    "\n",
    "Reseracher want to reduce the size of the BERT. They tried to reduce the size of BERT, and They made ALBERT, How lite it can be! \n",
    "\n",
    "The main Idea of ALBERT is to reduce the number of parameters(up to 90% reduction) using novel techniques while not taking a big hit to the performance. Now, this compressed version scales a lot better than the original BERT, improving the performance while still keeping the model small.\n",
    "\n",
    "\n",
    "**How it different from the normal BERT?**\n",
    "It uses three things that makes it seperate from the normal BERT. \n",
    "\n",
    "1. Factorized Embedding paramerter \n",
    "2. Cross Layer parameter Sharing \n",
    "3. Intersentece Co-Herance(contextual) Loss\n",
    "\n",
    "### Factorized Embedding Parameter: \n",
    "* In BERT, we give all the embedding to the model at a time. \n",
    "* Instead of giving all the embedding at a time, we can factorize the embedding. \n",
    "\n",
    "\n",
    "Example: \n",
    "`Original Data: Ed = 15, Factorized data: Ed = 3x5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16bfa4-65c5-475d-b889-dd50ea1e7c9b",
   "metadata": {},
   "source": [
    "* We are going to pass the data on small, small chunks, we pass the input but in factorized format. \n",
    "* Helps to reduce the memory constraint. \n",
    "\n",
    "**Why we are doing this:**\n",
    "* The number of parameter in hidden layer is larger than the input embeddings, so we ar breaking down it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff2c69-6572-4004-a383-5b5e63141186",
   "metadata": {},
   "source": [
    "### Cross Layer Parameter sharing: \n",
    "* In both the BERT large and small, we use differnt encoders sized like 6 and 12. \n",
    "* Some other models, exceeds more layers. \n",
    "* There is a drawback when there is very larger layer, each encoder layer initialize the weight in random way, all the encoders are independent in weight initilization. They don't have a connection between them during weight initialization it leads the model to **slow convergence**. \n",
    "* Reserached thinked, why can't we share the Initial layer(encoder) parameter to all the layers we have. \n",
    "* This is what they did, they just shared the **initial layer** parameters to the all the layers we have, so we get the weights from the inital layer, it helps the model for quicker convergence. \n",
    "* we propose cross-layer parameter sharing as another way to improve parameter efficiency. \n",
    "* **The default decision for ALBERT is to share all parameters across layers.**\n",
    "* It reduce the time also. \n",
    "\n",
    "\n",
    "### Inter Sentence Co-herence loss: \n",
    "* It's just a contextual relationship between the data. How sentences, characters, and words releated to each other(This is **co-herence**) \n",
    "* We maintain that inter-sentence modeling is an important aspect of language understanding, but we\n",
    "propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence\n",
    "coherence. T\n",
    "* While pretraining, we give 2 sentence to the model, how we can find the minimum loss between 2 sentence? (If I reduce this, the convergence time will be very fast) \n",
    "* It helps to predict the **Sentence order Prediction (SOP)**, If I find the next possible sentence, we can reduce the convergence time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390259d-c5e5-4379-9489-b6dc071002ef",
   "metadata": {},
   "source": [
    "**Some sort of difference:**\n",
    "<center><img src=\"https://iq.opengenus.org/content/images/2020/12/Screen-Shot-2020-12-12-at-3.29.47-AM.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "649fce14-621f-4aa2-bef7-caaaf5036b24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>Large</th>\n",
       "      <th>xlarge</th>\n",
       "      <th>XXLarge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parameter</th>\n",
       "      <td>12m</td>\n",
       "      <td>18million</td>\n",
       "      <td>60m</td>\n",
       "      <td>230million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Encoder</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Stage(attenion layer)</th>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>4038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input Size</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding size</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Base      Large xlarge     XXLarge\n",
       "parameter                     12m  18million    60m  230million\n",
       "Encoder                        12         24     24          12\n",
       "Hidden Stage(attenion layer)  768       1024   2048        4038\n",
       "Input Size                    128        128    128         128\n",
       "Embedding size                Yes        Yes    Yes         yes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dic = {'Base': ['12m', 12, 768, 128, 'Yes'], 'Large':['18million', 24, 1024, 128, 'Yes'], 'xlarge':['60m', 24, 2048, 128, 'Yes'], 'XXLarge':['230million', 12, 4038, 128, 'yes']} \n",
    "\n",
    "pd.DataFrame(dic, index = ['parameter', 'Encoder', 'Hidden Stage(attenion layer)', 'Input Size', 'Embedding size'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
