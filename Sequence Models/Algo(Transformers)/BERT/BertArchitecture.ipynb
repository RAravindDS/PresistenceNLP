{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9368ee1-9fb5-42cb-b04b-00b28ed3d81a",
   "metadata": {},
   "source": [
    "<center><h1>ğ“‘ğ“²-ğ“­ğ“²ğ“»ğ“®ğ“¬ğ“½ğ“²ğ“¸ğ“·ğ“ªğ“µ ğ“®ğ“·ğ“¬ğ“¸ğ“­ğ“®ğ“» ğ“»ğ“®ğ“¹ğ“»ğ“®ğ“¼ğ“®ğ“·ğ“½ğ“ªğ“½ğ“²ğ“¸ğ“· ğ“¯ğ“»ğ“¸ğ“¶ ğ“£ğ“»ğ“ªğ“·ğ“¼ğ“¯ğ“¸ğ“»ğ“¶ğ“®ğ“»ğ“¼</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96afdce-6ec8-4e71-bdb7-50fa249c4b3a",
   "metadata": {},
   "source": [
    "In Chapter 1, Getting Started with the Model Architecture of the Transformer, we defined \n",
    "the building blocks of the architecture of the original Transformer. Think of the \n",
    "original Transformer as a model built with LEGOÂ® bricks. The construction set \n",
    "contains bricks such as encoders, decoders, embedding layers, positional encoding \n",
    "methods, multi-head attention layers, masked multi-head attention layers, post-layer \n",
    "normalization, feed-forward sub-layers, and linear output layers. The bricks come \n",
    "in various sizes and forms. You can spend hours building all sorts of models using \n",
    "the same building kit! Some constructions will only require some of the bricks. Other \n",
    "constructions will add a new piece, just like when we obtain additional bricks for \n",
    "a model built using LEGOÂ® components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948d075-752c-4483-a40e-23e5f0e86edf",
   "metadata": {},
   "source": [
    "BERT added a new piece to the Transformer building kit: a bidirectional multi\u0002head attention sub-layer. When we humans are having problems understanding \n",
    "a sentence, we do not just look at the past words. BERT, like us, looks at all the \n",
    "words in the same sentence at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bc6f9-79b7-4483-b3cd-709af4e05fd6",
   "metadata": {},
   "source": [
    "**BERT only uses the blocks of the \n",
    "encoders of the Transformer in a novel way and does not use the decoder stack.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c76c0-cf11-438d-9dae-5238fe4bec75",
   "metadata": {},
   "source": [
    "We always do the **fine tunning**, **not pretraining**, because training takes several days, so we will download the pretrained model and we will fine tune for our task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428e05c-32fd-46a1-bcb7-6dd5f1b41821",
   "metadata": {},
   "source": [
    "## The architecture of BERT\n",
    "\n",
    "BERT introduces bidirectional attention to transformer models. Bidirectional \n",
    "attention requires many other changes to the original Transformer model.\n",
    "\n",
    "\n",
    "### The encoder stack \n",
    "\n",
    "<center><img src=\"images/encoder.png\" width=\"300\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a1b9f-317f-410c-8691-59d9c2bcfc5a",
   "metadata": {},
   "source": [
    "The dimension of the original transformer is: \n",
    "\n",
    "* N = 6 (Encoder & Decoder Layers) \n",
    "* d model = 512 (Dimensions) \n",
    "* A = 8 (Attentions Heads) \n",
    "\n",
    "<center><img src=\"images/form.png\" width=\"300\"/></center>\n",
    "\n",
    "**But the BERT is different from the orignial transformer** \n",
    "\n",
    "-----------------------------------------------------------------------**BERT SMALL**----------------------------------------------------------------------\n",
    "* It is also called BERT BASE. \n",
    "* N = 12 (Encoder Layers) \n",
    "* d model = 768 (Dimensions)\n",
    "* A = 12 (Attentions Heads) \n",
    "\n",
    "<center><img src=\"images/form2.png\" width=\"300\"/></center>\n",
    "\n",
    "-----------------------------------------------------------------------**BERT LARGE**----------------------------------------------------------------------\n",
    "* It is a BERT LARGE model. \n",
    "* N = 24 (Encoder Layers) \n",
    "* d model = 1024 (Dimensions)\n",
    "* A = 16 (Attentions Heads) \n",
    "\n",
    "<center><img src=\"images/form3.png\" width=\"300\"/></center>\n",
    "\n",
    "<center><img src=\"images/all.png\" width=\"800\"/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c99dd-2b32-420e-ab0f-e39d70b2a2ee",
   "metadata": {},
   "source": [
    "Size and dimensions play an essential role in BERT-style pretraining. BERT models \n",
    "are like humans. BERT models produce better results with more working memory \n",
    "(dimensions), and more knowledge (data). Large transformer models that learn large \n",
    "amounts of data will pretrain better for downstream NLP tasks.\n",
    "\n",
    "**ğŸ™‹ğŸ¾ More Dimensions More Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e873940-1402-4e6f-bc63-add73cd92072",
   "metadata": {},
   "source": [
    "### BERT Training Phase\n",
    "\n",
    "The BERT model has no decoder stack of layers. As such, it does not have a masked \n",
    "multi-head attention sub-layer. \n",
    "\n",
    "A masked multi-head attention layer masks all of the tokens that are beyond the \n",
    "present position. For example, take the following sentence:\n",
    "\n",
    "```\n",
    "The cat sat on it because it was a nice rug.\n",
    "```\n",
    "\n",
    "If we have just reached the word \"it,\" the input of the encoder could be:\n",
    "\n",
    "```\n",
    "The cat sat on it<masked sequence>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef69d2c-d559-4e14-8c42-ecd1764b26e1",
   "metadata": {},
   "source": [
    "The motivation of this approach is to prevent the model from seeing the output it is \n",
    "supposed to predict. This left-to-right approach produces relatively good results.\n",
    "However, the model cannot learn much this way. To know what \"it\" refers to, we \n",
    "need to see the whole sentence to reach the word \"rug\" and figure out that \"it\" was \n",
    "the rug.\n",
    "\n",
    "The authors of BERT came up with an idea. Why not pretrain the model to make \n",
    "predictions using a different approach?\n",
    "\n",
    "Ans: \n",
    "\n",
    "The authors of BERT came up with bidirectional attention, letting \n",
    "an attention head attend to all of the words both from left to right \n",
    "and right to left. To achieve this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd8ca4-ad4c-4451-aaea-77ada6a07a63",
   "metadata": {},
   "source": [
    "The model was trained with two tasks. The first method is **Masked Language\n",
    "Modeling (MLM)**. The second method is **Next Sentence Prediction (NSP)**. \n",
    "\n",
    "<center><img src=\"images/pass.png\" width=\"600\"/></center>\n",
    "\n",
    "**This MLM and NSP are perform simutaneously in the transformerğŸ‘¨ğŸ¼â€ğŸ¤**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260e548-b187-4b0a-9a7a-9e46b169f0a1",
   "metadata": {},
   "source": [
    "### 1.Masked Language Modeling: \n",
    "\n",
    "Bert introduces a **bi directional analysis** of a sentences with a random mask on a word on sentences, so we avoid a sequence of training. \n",
    "\n",
    "**Input**\n",
    "\n",
    "```\n",
    "\"The cat sat on it because it was a nice rug.\"\n",
    "```\n",
    "\n",
    "**Normal Transformer Masking**\n",
    "\n",
    "```\n",
    "\"The cat sat on it <masked sequence>.\"\n",
    "```\n",
    "\n",
    "**Bert Transformer Masking** \n",
    "\n",
    "```\n",
    "\"The cat sat on it [MASK] it was a nice rug.\"\n",
    "```\n",
    "\n",
    "The multi-attention sub-layer can now see the whole sequence, run the self-attention \n",
    "process, and predict the masked token.\n",
    "\n",
    "The input tokens were masked in a tricky way to force the model to train longer but \n",
    "produce better results with three methods:\n",
    "\n",
    "\n",
    "\n",
    "1. Surprise the model by not masking a single token on 10% of the dataset; for \n",
    "example:\n",
    "\n",
    "```\n",
    "\"The cat sat on it [because] it was a nice rug.\"\n",
    "```\n",
    "2. Surprise the model by replacing the token with a random token on 10% of the dataset; for example:\n",
    "\n",
    "```\n",
    "\"The cat sat on it [often] it was a nice rug.\"\n",
    "```\n",
    "\n",
    "3. Replace a token by a [MASK] token on 80% of the dataset; for example:\n",
    "\n",
    "```\n",
    "\"The cat sat on it [MASK] it was a nice rug.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805891a-64ba-43db-b089-5e01834dbbdc",
   "metadata": {},
   "source": [
    "### 2.Next Sentence Predictions \n",
    "\n",
    "It has two keywords **`[CLS]`** -> classifications, **`[SEP]`** -> seperator \n",
    "\n",
    "* [CLS] is a binary classification token added to the beginning of the first \n",
    "sequence to predict if the second sequence follows the first sequence. \n",
    "A positive sample is usually a pair of consecutive sentences taken from \n",
    "a dataset. A negative sample is created using sequences from different \n",
    "documents.\n",
    "* [SEP] is a separation token that signals the end of a sequence\n",
    "\n",
    "\n",
    "For example, the input sentences taken out of a book could be:\n",
    "```\n",
    "\"The cat slept on the rug. It likes sleeping all day.\"\n",
    "```\n",
    "These two sentences would become one input complete sequence:\n",
    "```\n",
    "[CLS] the cat slept on the rug [SEP] it likes sleep ##ing all day[SEP]\n",
    "```\n",
    "\n",
    "### Inputs and outputs of the BERT\n",
    "\n",
    "<center><img src=\"https://pytorch.org/tutorials/_images/bert.png\" width=\"600\"/></center>\n",
    "\n",
    "**C** -> Binary Output for **next Sentence Predictions** (0,1) \n",
    "\n",
    "**T** -> Outputs the **Word Vectors** of **Masked Language Model** (The nr of word vectors that we input as same as the number of word vectors we get as output)\n",
    "\n",
    "\n",
    "#### **`The input of the MLM and NSP needs some changes:`**\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Akbar-Karimi-4/publication/338934952/figure/fig2/AS:853247933808640@1580441568270/BERT-word-embedding-layer-Devlin-et-al-2018.ppm\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f84cc2-5e1c-401e-9ab1-d9674004bcc5",
   "metadata": {},
   "source": [
    "The input embeddings are obtained by summing the **token embeddings**, **positional embeddings** and the **segment (sentence/phrase) embeddings**. \n",
    "1. **`Token Embeddings`** are obtained by the **`WordPiece`** Algorithm. \n",
    "2. **`Sentence Embeeddings`** it's basically a sentence number encoded into vectors. \n",
    "3. **`Position Embeddings`** is position of a word within a sentence that is encoded into a vector. \n",
    "\n",
    "```Python\n",
    " [Position + Sentence + Token ] = [Embedding Vector]  -> Input to the bert\n",
    "```\n",
    "\n",
    "(The Segment and Position Embeddings required for **temporal ordering**, because all the inputs goes to the BERT we need some order). \n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Junyu-Lu-8/publication/343213861/figure/fig6/AS:981238156382211@1610956815537/BERT-with-Speaker-Segmentation-The-input-embeddings-are-the-sum-of-the-corresponding.ppm\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "#### Outputs of BERT \n",
    "\n",
    "<center><img src=\"images/final.png\" width=\"600\"/></center>\n",
    "\n",
    "**Benefits of Embeddings**\n",
    "\n",
    "* A sequence of words is broken down into WordPiece tokens **`(This is Token Embeddings)`** Token embedding uses the **`wordpiece`** algorithm.\n",
    "* A [MASK] token will randomly replace the initial word tokens for masked language modeling training. Fine-Tuning BERT Models\n",
    "* A [CLS] classification token is inserted at the beginning of a sequence for \n",
    "classification purposes.\n",
    "* A [SEP] token separates two sentences (segments, phrases) for NSP training.\n",
    "* Sentence embedding is added to token embedding, so that sentence A has a \n",
    "different sentence embedding value than sentence B.\n",
    "* Positional encoding is learned. The sine-cosine positional encoding method \n",
    "of the original Transformer is not applied.\n",
    "\n",
    "Some additional key features are:\n",
    "*  BERT uses bidirectional attention in all of its multi-head attention sub-layers, \n",
    "opening vast horizons of learning and understanding relationships between \n",
    "tokens.\n",
    "* BERT introduces scenarios of unsupervised embedding, pretraining models \n",
    "with unlabeled text. This forces the model to think harder during the \n",
    "multi-head attention learning process. This makes BERT able to learn how \n",
    "languages are built and apply this knowledge to downstream tasks without \n",
    "having to pretrain each time.\n",
    "*  BERT also uses supervised learning, covering all bases in the pretraining \n",
    "process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882cc229-d1a1-4602-9049-27c1075775ca",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bert.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff81c40-e41c-40a8-a744-1a39dd5ed9a8",
   "metadata": {},
   "source": [
    "**Pretraining is the first step of the BERT framework that can be broken down into two \n",
    "sub-steps:**\n",
    "\n",
    "* Defining the model's architecture: number of layers, number of heads,  dimensions, and the other building blocks of the model\n",
    "\n",
    "* Training the model on Masked Language Modeling (MLM) and NSP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb692e7-846c-40c0-9c91-7802bb19e19e",
   "metadata": {},
   "source": [
    " \n",
    "**The second step of the BERT framework is fine-tuning, which can also be broken down into two sub-steps:**\n",
    "\n",
    "* Initializing the downstream model chosen with the trained parameters of the \n",
    "  pretrained BERT model\n",
    "\n",
    "* Fine-tuning the parameters for specific downstream tasks such as \n",
    "  Recognizing Textual Entailment (RTE), Question Answering (SQuAD v1.1, \n",
    "  SQuAD v2.0), and Situations With Adversarial Generations (SWAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b43c48-543a-4f1f-ae81-435e64d09831",
   "metadata": {},
   "source": [
    "## Fine Tunning BERT\n",
    "\n",
    "[**You can refer this, because we don't have GPU. So, we are using (Collab)**](https://colab.research.google.com/drive/1LrUYTE6c77HvhZizw8I3sma7O9ui2hrz#scrollTo=R1GMyH21WgaW)\n",
    "\n",
    "[**Offical Notes, Previous My notes (Refer Both for better)**](https://github.com/PacktPublishing/Transformers-for-Natural-Language-Processing/blob/main/Chapter02/BERT_Fine_Tuning_Sentence_Classification_DR.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bd0de-1c95-4550-bb27-f86ce9eb1ac0",
   "metadata": {},
   "source": [
    "### Problems Solved By BERT \n",
    "* Translation \n",
    "* Question Answering \n",
    "* Sentiment Analysis\n",
    "* Text Summarization \n",
    "* MOre.... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
