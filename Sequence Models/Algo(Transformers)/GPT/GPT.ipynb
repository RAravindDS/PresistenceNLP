{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a677efd8-2b0f-4e5e-9953-915ee75c1ab3",
   "metadata": {},
   "source": [
    "**[GPT-3 Jay Allamar](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc88d-2405-48ec-bd79-01b692930348",
   "metadata": {},
   "source": [
    "After understanding the transformers, we should know that transformers are usefull to ner, pos and so on. The main aim of the transformer is that, it need to understand the input text fully (context vector). By utilizaing this architecuture people solved many problems by modifiying this architecutre. If you see the changed modified architecture you will find the first architecture is **GPT**\n",
    "\n",
    "When you understand the GPT you will encounter this two words \n",
    "1. Pre-Training \n",
    "2. Fine-Tunning \n",
    "\n",
    "**Pre-Training**: \n",
    "\n",
    "If you want to do any task, just consider you are Building a neural network instead of starting from random weight, you can start from some sorts of weights(trained by some other developers --pre-trained-weight--  ) it will be helpful to do better task right? Because it can recogonize pattern in your data easily, if you start from beginning we need to spent lot's of time to understand the pattern in our data. \n",
    "\n",
    "Same like google pulished the research paper **attention is all you need**, after this **open AI** build a architecture based on that(research paper) with very huge amount of data that arhitecutre is **GPT**. Instead of training our own model, we can use this pre build architecutre to recogonize pattern in our text. \n",
    "\n",
    "\n",
    "**Fine-Tunning**\n",
    "\n",
    "Pre-training is like some developers trained the weights, we will download the weights and use it -> pre-training \n",
    "\n",
    "Fine-Tunning is like after downloading the the pre-trained weights, we will pass our data and then, and then we will try to train our model for our specific task -> Fine-Tunning\n",
    "\n",
    "(You have some trained weights with you pass data on top of that which understand the relations on top of that you are trying to set own labeld or un labeld data for an specific task and then you are trying to adjust the outer layer weigths) -> Fine tunning \n",
    "\n",
    "**Not only here, if you are using BERT, are some other transformer, we will fine tune that, not pre train, we always fine tune the exist transformer**\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/5fbd459f3b05914cf70496d7/60cbd3ee2cff2abf2ae008b6_finetune.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463d097-fa9b-454a-9aaa-7f38224f0a33",
   "metadata": {},
   "source": [
    "There are lot's of version available in the GPT like GPT-1, GPT-2 .. \n",
    "\n",
    "**GPT** (first version) \n",
    "\n",
    "It has 12 layer **decoder network**. Please refer the research paper, you will get more knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66762a19-be62-4038-bd73-1d81dd4d12e8",
   "metadata": {},
   "source": [
    "### Let's Go \n",
    "\n",
    "GPT is nothing but (**Generative Pre-trained Transformer**). \n",
    "\n",
    "**First Step**: \n",
    "\n",
    "GPT firstly take the **un-supervised data** and it gives the data to **decoder based network**. \n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/gpt3/gpt3-training-examples-sliding-window.png\" width=\"600\"/>\n",
    "\n",
    "This approach is called **un-supervised approach**, it helps to train my weights and get pattern about the data. \n",
    "\n",
    "This is the **First set** of the **training process**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7c3c2-4166-445f-ba98-6bd5b595cffe",
   "metadata": {},
   "source": [
    "**Second step** \n",
    "\n",
    "Then we do the **Supervised learning**, now we have a label data with us, and our model trained with the unsupervised method before. \n",
    "\n",
    "<img src='https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif' width=\"600\"/>\n",
    "\n",
    "\n",
    "Please refer the research paper, more details are there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04247a56-028a-4921-bc80-830e856d9315",
   "metadata": {},
   "source": [
    "**Let's look more detail, how this process is happening** \n",
    "\n",
    "GPT actually generates output of one token at a time. \n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/gpt3/04-gpt3-generate-tokens-output.gif\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedadd88-b30c-462c-8df0-5a7d69c76048",
   "metadata": {},
   "source": [
    "**DrawBacks**: (GPT-1)\n",
    "\n",
    "* It is not bi directional, That's why we are using the **BERT**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
