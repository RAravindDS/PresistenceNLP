{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da95cb15-9dc1-4040-b5e3-0e6c65b3e18e",
   "metadata": {},
   "source": [
    "<h1><center>ğ“£ğ“»ğ“ªğ“·ğ“¼ğ“¯ğ“¸ğ“»ğ“¶ğ“®ğ“»ğ“¼</center></h1>\n",
    "\n",
    "[**ReserachPaper**](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "[**First Read this Then go to research paper**](http://jalammar.github.io/illustrated-transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386edf2-9009-4cbc-9fd0-f8445b02db99",
   "metadata": {},
   "source": [
    "The transformer is just basically a type of **machine learning** model, it just a type of **architecture** that contians **neural networks in it**. Google search engines uses the **transformer** for better results. \n",
    "\n",
    "The transformers are used in lots of deep learning purposes and this transformers are doing great job for NLP applications. The term **BERT** is also a transformer it's build top on that the normal transformer, The bert trasformer is doing great job comparing to other transformers. I not able to write good notes comparing to this blog, please check the blog, it's all about just 10 min reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a1401-bd75-4e78-9f6b-7b18ddf77363",
   "metadata": {},
   "source": [
    "We have sequence to sequence model, attention model then why transformer, when the sentence lenght increae, the accuracy got decrease, it is clearly mentioned in the research paper. So, we need to look at transformer. this is the state of art of NLP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
